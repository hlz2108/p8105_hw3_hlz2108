---
title: "HW 3 for P8105 - hlz2108"
author: "Helen Zhang"
date: "October 5, 2020"
output: github_document
---
Below is the code for setup:

```{r setup}
library(tidyverse)
library(dplyr)
library(readxl)
library(ggplot2)
library(p8105.datasets)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 0

This “problem” focuses on structure of your submission, especially the use git and GitHub for reproducibility, R Projects to organize your work, R Markdown to write reproducible reports, relative paths to load data from local files, and reasonable naming structures for your files.

## Problem 1

```{r instacart_df}
data("instacart")
```

This dataset contains `r nrow(instacart)` rows and `r ncol(instacart)` columns.

Observations are the level of items in order by user. 

How many aisles, and which are most items ordered from?

```{r aisle_count}
instacart %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

Let's make a plot!
This plot shows the number of items ordered in each aisle, limited to aisles with more than 10000 items ordered.

```{r instacart_plot}
instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_point() + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

Let's make a table!
This table shows the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits” and the number of times each item is ordered.

```{r instacart_table1}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable()
```

Apples vs. Ice-Cream
Making a second table.
This table shows the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week.

```{r instacart_table2}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarise(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```

## Problem 2

```{r accel_df}
accel_df = 
  read_csv("./data/accel_data.csv", col_types = cols(
      week = col_integer(),
      day_id = col_integer()
      )) %>%
  janitor::clean_names() %>% 
  mutate(
    weekend = case_when(
      day == "Monday" ~ "weekday",
      day == "Tuesday" ~ "weekday",
      day == "Wednesday" ~ "weekday",
      day == "Thursday" ~ "weekday",
      day == "Friday" ~ "weekday",
      day == "Saturday" ~ "weekend",
      day == "Sunday" ~ "weekend"
)) %>% 
  relocate("weekend") %>%
  pivot_longer(
    activity_1:activity_1440, 
    names_to = "activity",
    values_to = "activity_count") %>% 
  separate(activity, into = c("activity", "activity_minute")) %>%
  select(-activity) %>%
  mutate(
    activity_minute = as.numeric(activity_minute),
    day = factor(day),
    day = fct_relevel(day, c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))
    ) %>% view
```

The accel data set contains `r nrow(accel_df)` rows and `r ncol(accel_df)` columns. The dataset contains 5 weeks of accelerometer data collected from a 63 year-old male with BMI 25, who was admitted to the Advanced Cardiac Care Center of Columbia University Medical Center and diagnosed with congestive heart failure (CHF). The variables in this dataset are `r names(accel_df)`. Below is a detailed description of several variables of interest:

* _day_id_ is a `r class(pull(accel_df,day_id))` variable and indicates the day the accelerometer data was collected.

* _day_ is a `r class(pull(accel_df,day))` variable that was created in order to make the original `day_id` variable easier to understand.

* _week_ is a `r class(pull(accel_df, week))` variable and indicates the week the accelerometer data was collected.

* _weekend_ is a `r class(pull(accel_df, weekend))` variable that indicates whether the information was collected on a weekend vs. a weekday.

* _activity_minute_ is a `r class(pull(accel_df, activity_minute))` variable that indicates the minute when the activity count was collected, corresponding to each minute of a 24-hour day starting at midnight.

```{r aggregate_accel}
accel_df %>% 
  group_by(day_id, day, week) %>%
  summarize(total_activity = sum(activity_count)) %>% 
  knitr::kable()
```

Activity counts were at their lowest on the two Saturdays prior to end of data collection (weeks 4 and 5). The individual appears to be most active on the days leading up to and after the weekend, and during the weekend (Friday-Monday).

```{r accel_plot}
accel_df %>% 
  ggplot(aes(x = activity_minute, y = activity_count, color = day)) + 
  geom_line() +
  labs(
    title = "24-Hour Activity Count by Day",
    x = "Time",
    y = "Activity Count",
    caption = "Data from the accel dataset"
  ) + 
  scale_x_continuous(
    breaks = c(0, 360, 720, 1080, 1440), 
    labels = c("12AM", "6AM", "12PM", "6PM", "11:59PM"),
    limits = c(0, 1440)
    )
```

Activity is lowest during sleeping hours (12-6AM). The individual appears to be more active in the evening hours around 8-10pm, particularly on Fridays and Saturdays. On Sundays, they appear to most active at midday at 12pm.

## Problem 3

Loading NY_NOAA dataset.
```{r ny_noaa_df}
data("ny_noaa")
```

The NY_NOAA dataset contains information taken from the NOAA National Climatic Data Center. It contains information from all New York state weather stations from January 1, 1981 through December 31, 2010.
The NOAA dataset contains `r nrow(ny_noaa)` rows and `r ncol(ny_noaa)` columns.

The variables in the data set are:

* `id`: Weather station ID
* `date`: Date of observation
* `prcp`: Precipitation (tenths of mm)
* `snow`: Snowfall (mm)
* `snwd`: Snow depth (mm)
* `tmax`: Maximum temperature (tenths of degrees C)
* `tmin`: Minimum temperature (tenths of degrees C)

There are `r count(distinct(ny_noaa, id))` distinct weather stations in NYS.

Because not all weather stations in NY collect all these variables, the original dataset contains extensive amount of missing data.

To take a closer look at the amount of missing data, let's look at what proportion of data is missing for each variable.

```{r missing_noaa}
missing_noaa = ny_noaa %>% 
  summarize(
    missing_prcp = mean(is.na(prcp)),
    missing_snow = mean(is.na(snow)),
    missing_snwd = mean(is.na(snwd)),
    missing_tmax = mean(is.na(tmax)),
    missing_tmin = mean(is.na(tmin))
    ) %>% 
  knitr::kable()
```

From the table, we can see that there is quite a lot of missing data for `tmax` and `tmin`, up to 44% in each. `snow` and `snwd` have a smaller proportion of missing data. Missing data could influence the accuracy of further analysis, depending on when the missing data was collected.

#### Data Cleaning

```{r noaa_tidy}
noaa_tidy = ny_noaa %>% 
  janitor::clean_names() %>% 
  separate(date, into = c("year", "month", "day"), convert = TRUE) %>%
  mutate(
    prcp = prcp / 10 ,
    tmax = as.numeric(tmax) / 10,
    tmin = as.numeric(tmin) / 10,
    year = as.numeric(year),
    day = as.numeric(day),
    month = recode_factor(month,
          "01" = "January",
          "02" = "February",
          "03" = "March",
          "04" = "April",
          "05" = "May",
          "06" = "June",
          "07" = "July",
          "08" = "August",
          "09" = "September",
          "10" = "October",
          "11" = "November",
          "12" = "December"
    )) %>% 
  relocate(year, month, day, everything())
```

Several variables were converted into standard units. `prcp`, which was measured in tenths of mm, was converted into mm and `tmax` and `tmin`, which were measured in tenths of degrees C, was converted into degrees C.

```{r snowfall_noaa}
noaa_tidy %>% 
  count(snow) %>%
  arrange(desc(n))
```

From the dataframe above, you can see that the most commonly observed value for snowfall was 0, indicating that on most days in NYS, it is not snowing. This makes sense as snow is most likely to occur during the winter season (~3 months or so), and not year round.

```{r plot_noaa_tmax}
tmax_noaa = noaa_tidy %>% 
  filter(month %in% c("January", "July")) %>% 
  group_by(id, year, month) %>% 
  summarize(mean_tmax = mean(tmax, na.rm = TRUE))

ggplot(tmax_noaa, aes(x = year, y = mean_tmax, color = month)) +
  geom_point(alpha = 0.2) +
  geom_smooth(alpha = 0.5) +
  labs(
    title = "Avg Max Temp in January and July in each NY Station Across Years",
    x = "Year",
    y = "Temp (°C)",
    caption = "Data from the NOAA dataset"
  ) +
  scale_x_continuous(
    breaks = c(1980, 1985, 1990, 1995, 2000, 2005, 2010)
    ) +
  facet_grid(. ~ month) +
  scale_color_manual(values = c("navy", "orange")) +
  theme(
    legend.position = "none")
```

The average max temp in January is usually within the -10-10°C range. The lowest max temperature recorded was ~-14°C. There does not appear to be a significant different in average max temperatures throughout the years. By looking at the line generated from geom_smooth, one could say that there may be a slight increase in max temperatures when comparing 1980 vs. 2010 but it is difficult to determine whether there is any significant difference from the two-panel plot generated.

Average max temperatures in July appear to be in the 25-30°C range. There is one outlier where the max temp for July was ~14°C but generally speaking, most values are within the 25-30°C range. There does not appear to be a significant difference in average max temperatures in July throughout the years. However, 2010 did have one of the highest average temperatures, as high as ~33°C.

In comparing January vs. July, the average temps in January are substantially lower than in July. The max temps also appear to vary more in January, as the range is much broader/taller than in July, as seen on the plot (20 degrees vs 10 degrees).

```{r tmax_tmin_plot}
tmax_tmin_p = ggplot(noaa_tidy, aes(x = tmin, y = tmax)) +
  geom_hex(bins = 40) +
  geom_smooth(se = F)
  labs(
    title = "NY Temperatures (1981-2010)",
    x = "Min Temp (°C)",
    y = "Max Temp (°C)"
  ) + 
  theme(legend.position = "right")

tmax_tmin_p
```

```{r snowfall_p}
library(ggridges)

snowfall_p = noaa_tidy %>% 
  filter(snow > 0, snow < 100) %>% 
  ggplot(aes(x = snow, y = factor(year), fill = stat(x))) +
  geom_density_ridges_gradient(scale = 3, rel_min_height = 0.01) + 
  scale_fill_viridis_c(name = "Snowfall (mm)", option = "A") + 
  labs(
    title = "NY Snowfall Values, 0-100mm (1981-2010)",
    x = "Snowfall (mm)",
    y = "Year"
  )

snowfall_p
```

```{r patchwork_p}
tmax_tmin_p + snowfall_p
```

The left hand side shows `tmax` vs `tmin` for the full dataset. It appears that most days have a maximum temperature of ~25°C and a minimum temperature of ~15°C. The line helps the reader visually see the most common max and min temps for days. The relationship appears to be linear in the middle but slightly sigmoidal (S-shaped) at the ends.

The right hand side shows the distribution of snowfall values that are greater than 0 and less than 100 separately by year.
Each boxplot represents a different year. The snowfall distribution does not appear to vary significantly by year, and it is difficult to determine any trends from this plot.